## 1.数据预处理  
在自然语言处理任务中，我们首先需要考虑的是：计算机是无法直接识别任务文字的（包括:中文、英文、德文等等），**“词如何在计算机中表示”** 是任何自然语言处理任务中的 **“入门券”**。

### 1.1 One-hot 编码  
One-hot 编码，又叫做独热编码，其是一种将数据转换为数值数据的技术；每个类别用一个位图来表示，该位图中只有一个位置的值为1，其余位置的值为0。  

比如：“我”，“爱”，“你”这三个单词，One-hot 编码表示如下：   

| 我      | 1  | 0  | 0  |
|---------|----|----|----|
| 爱      | 0  | 1  | 0  |
| 你      | 0  | 0  | 1  |

但这样做，One-hot的维度是由词库（单词的数量）来决定的，如果是10000个单词，One-hot维度就须是10000维，  
这往往会导致两个问题：  
- 维度灾难，随着数据维度的增加，导致数据变得非常稀疏；这使得寻找相似点或进行聚类等变得困难。
- 无法度量每个词语之间的相似度，就比如："Peking University"和"北京大学"表示的都是同一个意思，如果用One-hot 编码，并不能够表示出两个词汇之间的相似度，无法度量。


### 1.2 Word2Vec 词嵌入

为了解决One-hot 编码的困境，谷歌团队提出了Word2Vec的方法。  
**Word2Vec** 是一种用于将单词转换为向量表示的方法，该方法旨在通过从大量文本数据中学习，生成能够捕捉单词语义信息的向量，使得在向量空间中，**相似意义的词距离更近**。  

简单地说，12维度的数据，在One-hot 编码中只能表示12个单词，而在Word2Vec中，可以表示无数个单词。 
通常，数据的维度越高，能提供的信息量也就越大，一般数据的维度我们控制在 **[50, 100]** 之间。

要准确地将单词进行向量化表示，则需要考虑两件事：
- 单词 (单个token) 本身的含义
- 单词和其他单词之间的关联或关系或者相似度

单词本身的含义一般是很容易就可以做到的，难点在于如何将一个单词和其他单词进行关联；Word2Vec主要是提出了两种方案：  
- CBOW: 根据上下文词预测中心词
- Skip-gram: 根据中心词预测上下文词

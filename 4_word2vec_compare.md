## 3. é€šè¿‡è¯è¢‹æ¨¡å‹ã€TF-IDFæ¨¡å‹ã€è¯å‘é‡æ¨¡å‹ã€æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†åˆ«è¿›è¡Œè¯åµŒå…¥ï¼Œæ¥æŸ¥çœ‹ä»–ä»¬çš„åŒºåˆ«ã€‚

æœ¬èŠ‚é‡Œæ‰€ç”¨çš„æ•°æ®é›†å’Œæ–‡ä»¶å‡å¯åœ¨ä¸‹é¢è·å–ï¼š  
[socialmedia_relevant_cols_clean.csv](https://ww0.lanzout.com/iU6Ux276hpeh)  
[cleaned_data.csv](https://ww0.lanzout.com/icAWk276hvuj)


### 3.1 æ•°æ®é¢„å¤„ç†
```python
import keras
import nltk
import pandas as pd
import numpy as np
import re
import codecs
```
```python
questions = pd.read_csv("socialmedia_relevant_cols_clean.csv")
questions.head()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>text</th>
      <th>choose_one</th>
      <th>class_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>just happened a terrible car crash</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>our deeds are the reason of this  earthquake m...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>heard about  earthquake is different cities, s...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>there is a forest fire at spot pond, geese are...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>forest fire near la ronge sask  canada</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

```python
questions.describe()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>class_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>count</th>
      <td>10876.000000</td>
      <td>10876.000000</td>
    </tr>
    <tr>
      <th>mean</th>
      <td>5437.500000</td>
      <td>0.432604</td>
    </tr>
    <tr>
      <th>std</th>
      <td>3139.775098</td>
      <td>0.498420</td>
    </tr>
    <tr>
      <th>min</th>
      <td>0.000000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>25%</th>
      <td>2718.750000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>50%</th>
      <td>5437.500000</td>
      <td>0.000000</td>
    </tr>
    <tr>
      <th>75%</th>
      <td>8156.250000</td>
      <td>1.000000</td>
    </tr>
    <tr>
      <th>max</th>
      <td>10875.000000</td>
      <td>2.000000</td>
    </tr>
  </tbody>
</table>
</div>

**æ•°æ®æ¸…æ´—ï¼Œå»æ‰æ— ç”¨å­—ç¬¦**
```python
def standardize_text(df, text_field):
    df[text_field] = df[text_field].str.replace(r"http\S+", "")
    df[text_field] = df[text_field].str.replace(r"http", "")
    df[text_field] = df[text_field].str.replace(r"@\S+", "")
    df[text_field] = df[text_field].str.replace(r"[^a-zA-Z0-9(),!?@\'\`\"\_\n]", "")
    df[text_field] = df[text_field].str.replace(r"@", "at")
    df[text_field] = df[text_field].str.lower()
    return df

questions = standardize_text(questions, "text")

questions.to_csv("cleaned_data.csv")
questions.head()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0</th>
      <th>text</th>
      <th>choose_one</th>
      <th>class_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>just happened a terrible car crash</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>our deeds are the reason of this  earthquake m...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>heard about  earthquake is different cities, s...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>there is a forest fire at spot pond, geese are...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>forest fire near la ronge sask  canada</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

```python
clean_data = pd.read_csv("cleaned_data.csv")
clean_data.head()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Unnamed: 0.1</th>
      <th>Unnamed: 0</th>
      <th>text</th>
      <th>choose_one</th>
      <th>class_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>just happened a terrible car crash</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>1</td>
      <td>1</td>
      <td>our deeds are the reason of this  earthquake m...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>2</td>
      <td>2</td>
      <td>heard about  earthquake is different cities, s...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>3</td>
      <td>3</td>
      <td>there is a forest fire at spot pond, geese are...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>4</td>
      <td>4</td>
      <td>forest fire near la ronge sask  canada</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

```python
# ä¸Šé¢å¯ä»¥çœ‹å‡ºæœ‰äº›åˆ—é‡å¤äº†ï¼Œè¿›è¡Œåˆ é™¤
clean_data = clean_data.drop(columns=['Unnamed: 0.1'])
clean_data = clean_data.drop(columns=['Unnamed: 0'])

clean_data.head()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>choose_one</th>
      <th>class_label</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>just happened a terrible car crash</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>1</th>
      <td>our deeds are the reason of this  earthquake m...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>2</th>
      <td>heard about  earthquake is different cities, s...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>3</th>
      <td>there is a forest fire at spot pond, geese are...</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
    <tr>
      <th>4</th>
      <td>forest fire near la ronge sask  canada</td>
      <td>Relevant</td>
      <td>1</td>
    </tr>
  </tbody>
</table>
</div>

**æŸ¥çœ‹æ•°æ®åˆ†å¸ƒæƒ…å†µ: æ˜¯å¦ä¸å¹³è¡¡  **
```python
clean_data.groupby("class_label").count()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>choose_one</th>
    </tr>
    <tr>
      <th>class_label</th>
      <th></th>
      <th></th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>6187</td>
      <td>6187</td>
    </tr>
    <tr>
      <th>1</th>
      <td>4673</td>
      <td>4673</td>
    </tr>
    <tr>
      <th>2</th>
      <td>16</td>
      <td>16</td>
    </tr>
  </tbody>
</table>
</div>

**åˆ†è¯ã€æ•°æ®é›†åˆ’åˆ†  **
```python
# åˆ†è¯
# ä½¿ç”¨äº†NLTKåº“ä¸­çš„RegexpTokenizeræ¥å¯¹æ–‡æœ¬è¿›è¡Œåˆ†è¯
from nltk.tokenize import RegexpTokenizer

tokenizer = RegexpTokenizer(r'\w+')

clean_data["tokens"] = clean_data["text"].apply(tokenizer.tokenize)
clean_data.head()
```
<div>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>text</th>
      <th>choose_one</th>
      <th>class_label</th>
      <th>tokens</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>just happened a terrible car crash</td>
      <td>Relevant</td>
      <td>1</td>
      <td>[just, happened, a, terrible, car, crash]</td>
    </tr>
    <tr>
      <th>1</th>
      <td>our deeds are the reason of this  earthquake m...</td>
      <td>Relevant</td>
      <td>1</td>
      <td>[our, deeds, are, the, reason, of, this, earth...</td>
    </tr>
    <tr>
      <th>2</th>
      <td>heard about  earthquake is different cities, s...</td>
      <td>Relevant</td>
      <td>1</td>
      <td>[heard, about, earthquake, is, different, citi...</td>
    </tr>
    <tr>
      <th>3</th>
      <td>there is a forest fire at spot pond, geese are...</td>
      <td>Relevant</td>
      <td>1</td>
      <td>[there, is, a, forest, fire, at, spot, pond, g...</td>
    </tr>
    <tr>
      <th>4</th>
      <td>forest fire near la ronge sask  canada</td>
      <td>Relevant</td>
      <td>1</td>
      <td>[forest, fire, near, la, ronge, sask, canada]</td>
    </tr>
  </tbody>
</table>
</div>

**è¯­æ–™åº“æƒ…å†µ  **
```python
from keras_preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
```

```python
# ä¸€å…±æœ‰å¤šå°‘ä¸ªå•è¯
all_words = [word for tokens in clean_data["tokens"] for word in tokens]
# æ¯ä¸ªå¥å­çš„é•¿åº¦
sequences_lengths = [len(tokens) for tokens in clean_data["tokens"]]
# è¯­æ–™åº“åŒ…å«å¤šå°‘ä¸ªä¸åŒçš„å•è¯
VOCAB = sorted(list(set(all_words)))
print("%s words total, with a vocabluary size of %s" % (len(all_words), len(VOCAB)))
print("Max sentence length is %s" % max(sequences_lengths))
```
154724 words total, with a vocabluary size of 18101  
Max sentence length is 34  

```python
import matplotlib.pyplot as plt

fig = plt.figure(figsize=(10,10))
plt.xlabel("Sentence length")
plt.ylabel("Number of sentences")
plt.hist(sequences_lengths)
plt.show()
```
![image](https://github.com/user-attachments/assets/b3529fe7-623d-4e70-9a3d-01a7b9ec13ef)

ä¸‹é¢é€šè¿‡è¯è¢‹æ¨¡å‹ã€TF-IDFæ¨¡å‹ã€è¯å‘é‡æ¨¡å‹ã€æ·±åº¦å­¦ä¹ æ¨¡å‹åˆ†åˆ«è¿›è¡Œè¯åµŒå…¥ï¼Œæ¥æŸ¥çœ‹ä»–ä»¬çš„åŒºåˆ«ã€‚

### 3.2 è¯è¢‹æ¨¡å‹ Bag of Words Counts 
```python
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

def cv(data):
    count_vectirizer = CountVectorizer()
    emb = count_vectirizer.fit_transform(data)
    return emb, count_vectirizer

# å°†clean_data["text"]æ‰€æœ‰å†…å®¹éƒ½æ”¾åœ¨ä¸€ä¸ªåˆ—è¡¨ä¸­
list_corpus = clean_data["text"].tolist()
list_labels = clean_data["class_label"].tolist()

X_train, X_test, y_train, y_test = train_test_split(list_corpus, list_labels, test_size=0.2, random_state=32)

X_train_counts, count_vectorizer = cv(X_train)
X_test_counts = count_vectorizer.transform(X_test)
```

**PCAå±•ç¤ºBag of Words  **
```python
from sklearn.decomposition import PCA, TruncatedSVD
import matplotlib
import matplotlib.patches as mpatches

def plot_LSA(test_data, test_labels, savepath="PCA_demo.csv", plot=True):
    lsa = TruncatedSVD(n_components=2)
    lsa.fit(test_data)
    lsa_scores = lsa.transform(test_data)
    color_mapper = {label: idx for idx, label in enumerate(set(test_labels))}
    color_column = [color_mapper[label] for label in test_labels]
    colors = ['orange', 'blue', 'blue']
    if plot:
        plt.scatter(lsa_scores[:,0], lsa_scores[:,1], s=8, alpha=.8, c=test_labels, cmap=matplotlib.colors.ListedColormap(colors))
        red_patch = mpatches.Patch(color='orange', label='Irrelevant')
        green_patch = mpatches.Patch(color='blue', label='Disaster')
        plt.legend(handles=[red_patch, green_patch], prop={'size': 30})

fig = plt.figure(figsize=(16, 16))
plot_LSA(X_train_counts, y_train)
plt.show()
```
![image](https://github.com/user-attachments/assets/c413e3c8-3a39-4243-8b69-364f4cf4ce30)

**å¯ä»¥çœ‹æ¥PCAæ•ˆæœå¹¶ä¸æ˜¯å¾ˆå¥½  **

**é€»è¾‘å›å½’åˆ†ç±»çœ‹ä¸€ä¸‹ç»“æœ**
```python
from sklearn.linear_model import LogisticRegression

clf = LogisticRegression(C=30.0, class_weight='balanced', solver="newton-cg",
                         multi_class='multinomial', n_jobs=-1, random_state=40)
clf.fit(X_train_counts, y_train)

y_predicted_count = clf.predict(X_test_counts)
```

```python
# è¯„ä¼°
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report

def get_metrics(y_test, y_predicted):
    precision = precision_score(y_test, y_predicted, pos_label=None, average='weighted')
    recall = recall_score(y_test, y_predicted, pos_label=None, average='weighted')
    f1 = f1_score(y_test, y_predicted, pos_label=None, average='weighted')

    accuracy = accuracy_score(y_test, y_predicted)
    return accuracy, precision, recall, f1

accuracy, precision, recall, f1 = get_metrics(y_test, y_predicted_count)
print("accuracy: %.3f, precision: %.3f, recall: %.3f, f1: %.3f" % (accuracy, precision, recall, f1))
```
accuracy: 0.769, precision: 0.771, recall: 0.769, f1: 0.769  

```python
# æ··æ·†çŸ©é˜µ
import numpy as np
import itertools
from sklearn.metrics import confusion_matrix

def plot_confusion_matrix(cm, 
                          classes, 
                          title="Confusion matrix", 
                          cmap=plt.cm.Blues):
    plt.imshow(cm, interpolation='nearest',cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.xticks(tick_marks, classes)

    thresh = cm.max() / 2
    for i,j in itertools.product(range(cm.shape[0]),range(cm.shape[1])):
        plt.text(j, i, cm[i,j],
                 horizontalalignment="center",
                 color="white" if cm[i,j] > thresh else "black")
    
    plt.tight_layout()
    plt.ylabel("True label")
    plt.xlabel("Predicted label")
```

```python
cm = confusion_matrix(y_test, y_predicted_count)
fig = plt.figure(figsize=(10,10))
plot = plot_confusion_matrix(cm, classes=['Irrelevant', 'Disaster', 'Unsure'], title="Confusion Matrix")
plt.show()
```
![image](https://github.com/user-attachments/assets/54778841-f3d7-4241-bd03-6aebfe3f759d)

```python
print(cm)
```
[[1001  210    5]
 [ 283  673    2]
 [   2    0    0]]

**è¿›ä¸€æ­¥æ£€æŸ¥æ¨¡å‹çš„å…³æ³¨ç‚¹  **
```python
def get_most_important_features(vectorizer, model, n=5):
    index_to_word = {v:k for k,v in vectorizer.vocabulary_.items()}
    
    classes = {}
    for class_index in range(model.coef_.shape[0]):
        word_importances = [(el, index_to_word[i]) for i,el in enumerate(model.coef_[class_index])]
        sorted_coeff = sorted(word_importances, key = lambda x: x[0], reverse=True)
        tops = sorted(sorted_coeff[:n], key=lambda x: x[0])
        bottom = sorted_coeff[-n:]
        classes[class_index] = {
            'tops':tops,
            'bottom': bottom
        }
        return classes

importance = get_most_important_features(count_vectorizer, clf, 10)
importance
```
{0: {'tops': [(2.115824123340634, 'windy'),
   (2.1799153731543583, 'weighs'),
   (2.2054055038586355, 'ice'),
   (2.215906647191191, 'age'),
   (2.259999163387926, 'breaks'),
   (2.2744918042282802, 'christmas'),
   (2.2827571787082026, 'poll'),
   (2.5208192943590193, 'swell'),
   (3.1090845052883997, 'finally'),
   (3.545381511454468, 'po')],
  'bottom': [(-2.8219688769947275, 'x1392'),
   (-3.0023705607413107, 'typhoon'),
   (-3.021015416493898, 'sunburned'),
   (-3.080252298531729, 'derailment'),
   (-3.091500326132359, 'distance'),
   (-3.2005696202249267, 'ryans'),
   (-3.53937836422686, 'stake'),
   (-3.5881606223436653, 'hiroshima'),
   (-4.747463021900055, 'deaths'),
   (-4.884968504327644, 'storm')]}}  

```python
def plot_important_words(top_scores,top_words,bottom_scores,bottom_words,name):
    y_pos = np.arange(len(top_words))
    top_pairs =[(a,b)for a,b in zip(top_words, top_scores)]
    top_pairs=sorted(top_pairs, key=lambda x:x[1])
    
    bottom_pairs =[(a,b)for a,b in zip(bottom_words, bottom_scores)]
    bottom_pairs=sorted(bottom_pairs, key=lambda x: x[1], reverse=True)
   
    top_words = [a[0] for a in top_pairs]
    top_scores =[a[1] for a in top_pairs]

    bottom_words =[a[0] for a in bottom_pairs]
    bottom_scores =[a[1] for a in bottom_pairs]

    fig = plt.figure(figsize=(10, 10))
    plt.subplot(121)
    plt.barh(y_pos, bottom_scores, align='center', alpha=0.5)
    plt.title('Irrelevant', fontsize=20)
    plt.yticks(y_pos, bottom_words, fontsize=14)
    plt.suptitle('Key words' , fontsize=16)
    plt.xlabel('Importance', fontsize=20)

    plt. subplot(122)
    plt.barh(y_pos,top_scores, align='center', alpha=0.5)
    plt.title('Disaster', fontsize=20)
    plt.yticks(y_pos, top_words, fontsize=14)
    plt.suptitle(name, fontsize=16)
    plt.xlabel('Importance' , fontsize=20)
```

```python
top_scores = [a[0] for a in importance[0]['tops']]
top_words = [a[1] for a in importance[0]['tops']]

bottom_scores = [a[0] for a in importance[0]['bottom']]
bottom_words = [a[1] for a in importance[0]['bottom']]

plot_important_words(top_scores, top_words, bottom_scores, bottom_words, "Most important words for relevance")
```
![image](https://github.com/user-attachments/assets/0c71f6d1-7109-4b70-9450-10cde97ade5d)


```python
import numpy as np

def get_precision_recall(pred_labels, ground_labels):
    pred_labels = np.reshape(pred_labels, (-1, 1))
    ground_labels = np.reshape(ground_labels, (-1, 1))
    classification = np.sum(np.equal(pred_labels, ground_labels)) / (
                ground_labels.shape[0] * ground_labels.shape[1]) * 100
    true_results = pred_labels[ground_labels == 1]

    ########################################
    TP = np.sum(true_results)
    epsilon = 1e-7  # Small value to avoid division by zero
    precision = TP / (np.sum(pred_labels[pred_labels == 1]) + epsilon) * 100
    recall = TP / (np.sum(ground_labels[ground_labels == 1]) + epsilon) * 100

    ###########################################

    print('accu:%.2f, precision:%.2f, recall=%.2f' % (classification, precision, recall))
    return classification, precision, recall

# Example usage
pred_labels = np.array([[1, 0, 1], [0, 1, 1], [1, 1, 0]])
ground_labels = np.array([[1, 0, 1], [0, 1, 0], [1, 1, 1]])
get_precision_recall(pred_labels, ground_labels)
```
accu:77.78, precision:83.33, recall=83.33  
(77.77777777777779, 83.33333194444447, 83.33333194444447)


### 3.3 TF-IDF Bag of Words

TF-IDF æ˜¯ä¸€ç§ç”¨äºä¿¡æ¯æ£€ç´¢ä¸æ–‡æœ¬æŒ–æ˜çš„å¸¸ç”¨åŠ æƒæŠ€æœ¯ï¼Œç”¨æ¥è¯„ä¼°ä¸€ä¸ªè¯å¯¹äºä¸€ä¸ªæ–‡æ¡£é›†æˆ–è¯­æ–™åº“ä¸­æŸä¸ªæ–‡æ¡£çš„é‡è¦ç¨‹åº¦ã€‚    
 
ä¾‹å¦‚ï¼šã€Šä¸­å›½çš„èœœèœ‚å…»æ®–ã€‹è¿™æœ¬ä¹¦ï¼Œç»è¿‡æ•°æ®ç»Ÿè®¡åï¼Œä¸­å›½ã€èœœèœ‚ã€å…»æ®–è¿™ä¸‰ä¸ªè¯çš„æ•°é‡æœ€å¤šï¼Œé‚£ä¹ˆè°èƒ½ä»£è¡¨è¿™ç¯‡æ–‡ç« çš„é‡è¦æ€§å‘¢ï¼Ÿ    
ä¸­å›½ä¸€èˆ¬æ˜¯æ¯”è¾ƒå¸¸è§çš„ï¼Œé‚£ä¹ˆæ˜¯â€œèœœèœ‚â€è¿˜æ˜¯â€œå…»æ®–â€èƒ½å¤Ÿä»£è¡¨è¿™æœ¬ä¹¦çš„ç‰¹å¾å‘¢ï¼Ÿ

$è¯é¢‘(TF) = \frac{æŸä¸ªè¯ t åœ¨æ–‡ç«  d ä¸­å‡ºç°çš„æ¬¡æ•°}{æ–‡ç«  d çš„æ€»è¯æ•°}$  
$é€†æ–‡æ¡£é¢‘ç‡(IDF) = log \frac{è¯­æ–™åº“ D çš„æ–‡æ¡£æ€»æ•°}{åŒ…å«è¯ t çš„æ–‡æ¡£æ•°é‡ + 1}$  
$ TF-IDF = TF \cdot IDF$ï¼Œ  

è§£é‡Šï¼šå½“è¯é¢‘ TF æ•°é‡è¶Šå¤§ï¼Œä¸” IDF è¶Šå¤§ï¼Œ åˆ™è¯´æ˜è¿™ä¸ªè¯æ±‡çš„é‡è¦æ€§ç¨‹åº¦è¶Šé«˜ï¼›IDF è¶Šå¤§ï¼Œå°±è¯´æ˜åŒ…å«è¯¥è¯çš„æ–‡æ¡£æ•°é‡è¶Šå°‘ã€è¶Šç¨€æœ‰ï¼Œå°±è¯´æ˜è¯¥è¯è¶Šçè´µï¼ï¼

```python
def tfidf(data):
    tfidf_vevtorizer = TfidfVectorizer() # å®ƒç”¨äºå°†æ–‡æœ¬è½¬æ¢ä¸ºTF-IDFç‰¹å¾çŸ©é˜µã€‚
    '''
    fit_transform æ–¹æ³•å¯¹ data è¿›è¡Œå­¦ä¹ ï¼ˆå³æ ¹æ®æ–‡æœ¬æ•°æ®ä¸­çš„è¯æ±‡æ„å»ºè¯æ±‡è¡¨ï¼‰ï¼Œ
    ç„¶åå°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºTF-IDFç‰¹å¾çŸ©é˜µã€‚è¿”å›çš„ train æ˜¯ä¸€ä¸ªç¨€ç–çŸ©é˜µï¼ŒåŒ…å«äº†æ–‡æœ¬æ•°æ®çš„TF-IDFç‰¹å¾ã€‚
    '''
    train = tfidf_vevtorizer.fit_transform(data) 
    return train, tfidf_vevtorizer

# å¯¹è®­ç»ƒæ•°æ® X_train è¿›è¡ŒTF-IDFç‰¹å¾æå–ï¼Œå¹¶å¾—åˆ°ç›¸åº”çš„ç‰¹å¾çŸ©é˜µ X_train_tfidf
X_train_tfidf, tfidf_vectorizer = tfidf(X_train)
'''
ä½¿ç”¨ç›¸åŒçš„TF-IDFå‘é‡åŒ–å™¨ tfidf_vectorizer å¯¹æµ‹è¯•æ•°æ® X_test è¿›è¡Œè½¬æ¢ï¼Œå¾—åˆ°æµ‹è¯•æ•°æ®çš„ç‰¹å¾çŸ©é˜µ X_test_tfidfã€‚
è¿™æ ·å¯ä»¥ç¡®ä¿è®­ç»ƒæ•°æ®å’Œæµ‹è¯•æ•°æ®åœ¨ç›¸åŒçš„è¯æ±‡è¡¨å’Œè½¬æ¢æ–¹å¼ä¸‹å¾—åˆ°ç‰¹å¾è¡¨ç¤ºã€‚
'''
X_test_tfidf = tfidf_vectorizer.transform(X_test)
```

```python
fig = plt.figure(figsize=(16, 16))
plot_LSA(X_train_tfidf, y_train)
plt.show()
```
![image](https://github.com/user-attachments/assets/8e4253c0-1f6d-465c-b819-d3f0dd42a06a)

**å’Œè¯è¢‹æ¨¡å‹ç›¸æ¯”ï¼ŒTF-IDFçš„æ•ˆæœçœ‹ä¸Šå»ä¼šç¨å¾®å¥½ä¸€ç‚¹  **

```python
clf_tfidf = LogisticRegression(C=30.0, class_weight='balanced', solver="newton-cg",
                         multi_class='multinomial', n_jobs=-1, random_state=40)
clf_tfidf.fit(X_train_tfidf, y_train)

y_predicted_tfidf = clf_tfidf.predict(X_test_tfidf)
```

```python
accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf = get_metrics(y_test, y_predicted_tfidf)
print("accuracy: %.3f, precision: %.3f, recall: %.3f, f1: %.3f" % (accuracy_tfidf, precision_tfidf, recall_tfidf, f1_tfidf))
```
accuracy: 0.773, precision: 0.773, recall: 0.773, f1: 0.773  

```python
cm = confusion_matrix(y_test, y_predicted_tfidf)
fig = plt.figure(figsize=(10,10))
plot = plot_confusion_matrix(cm, classes=['Irrelevant', 'Disaster', 'Unsure'], title="Confusion Matrix")
plt.show()
```
![image](https://github.com/user-attachments/assets/5d8f8245-d06a-44a1-b47f-9b1cb48cb586)

```python
importance_tfidf = get_most_important_features(tfidf_vectorizer, clf_tfidf, 10)
```

```python
top_scores = [a[0] for a in importance_tfidf[0]['tops']]
top_words = [a[1] for a in importance_tfidf[0]['tops']]

bottom_scores = [a[0] for a in importance_tfidf[0]['bottom']]
bottom_words = [a[1] for a in importance_tfidf[0]['bottom']]

plot_important_words(top_scores, top_words, bottom_scores, bottom_words, "Most important words for relevance")
```
![image](https://github.com/user-attachments/assets/909b1970-1fdf-4f8e-a640-8ea13c589c9d)

**ä½†æ˜¯ï¼Œä»¥ä¸Šæ–¹æ³•éƒ½æ˜¯è€ƒè™‘æ¯ä¸€ä¸ªåŸºäºé¢‘ç‡çš„æƒ…å†µï¼Œå¦‚æœåœ¨æ–°çš„æµ‹è¯•ç¯å¢ƒä¸‹æœ‰äº›è¯å‘ç”Ÿå˜åŒ–äº†æ€ä¹ˆåŠï¼Ÿ  

æ¯”å¦‚ï¼šæœ‰ä¸€äº›å•è¯ä»æ¥éƒ½æ²¡æœ‰åœ¨ä¹‹å‰çš„æ–‡æ¡£é‡Œå‡ºç°è¿‡   
æˆ–è€…  
ç±»ä¼¼äºbadå’Œworseçš„å•è¯è¡¨è¾¾çš„æ„ä¹‰å·®ä¸å¤šä½†æ˜¯é•¿å¾—ä¸ä¸€æ ·  

è¯é¢‘ç»Ÿè®¡çš„æ–¹æ³•å°±æ— æ³•æˆ–è€…å¾ˆéš¾æ•æ‰åˆ°è¿™äº›ç‰¹å¾ã€‚  **

### 3.4 word2vec
```python
import gensim

# https://huggingface.co/NathaNn1111/word2vec-google-news-negative-300-bin/tree/main
word2vec_path = "GoogleNews-vectors-negative300.bin"
word2vec = gensim.models.KeyedVectors.load_word2vec_format(word2vec_path, binary=True)
```

```python
def get_average_word2vec(token_list, vector, generate_missing=False, k=300):
    if len(token_list)<1:
        return np.zeros(k)
    if generate_missing:
        vectorized = [vector[word] if word in vector else np.random.rand(k) for word in token_list]
    else:
        vectorized = [vector[word] if word in vector else np.zeros(k) for word in token_list]
    length = len(vectorized)
    summed = np.sum(vectorized, axis=0)
    averaged = np.divide(summed, length)
    return averaged

def get_word2vec_embeddings(vectors, clean_question, generate_missing=False):
    embeddings = clean_question['tokens'].apply(lambda x: get_average_word2vec(x, vectors, generate_missing=generate_missing))
    return list(embeddings)
```

```python
embeddings = get_word2vec_embeddings(word2vec, clean_data)

X_train_word2vec, X_test_word2vec, y_train_word2vec, y_test_word2vec = train_test_split(embeddings, list_labels,
                                                                                        test_size=0.2, random_state=40)
```

```python
X_train_word2vec[2]
```
array([-1.67142428e-03,  5.19831731e-02, -4.97671274e-04,  9.55153245e-02,
       -1.56419020e-01, -1.42728365e-03,  4.40603403e-02, -9.99380258e-02,
        3.44191331e-02,  5.22085337e-02, -1.76591140e-02, -1.42493615e-01,
       -1.30690355e-01,  9.65951773e-02, -9.96610201e-02,  1.47047776e-01,
        1.26600999e-01,  1.73612154e-01,  3.83347731e-03, -3.90413724e-02,
       -8.86922983e-02, -6.96927584e-02,  8.54926476e-02, -4.00108924e-02,
        8.65689791e-02, -2.40478516e-02, -1.22312106e-01, -2.58166973e-02,
       -5.26310847e-03, -7.98105093e-02, -3.84005033e-02,  2.73719201e-02,
       -1.58835191e-02, -3.35364709e-02, -4.36589168e-02,  1.90523588e-02,
       -4.92351239e-02,  8.82943960e-02,  1.04041466e-01,  9.55528846e-02,
        1.38624925e-01, -2.42991814e-02,  1.56306340e-01,  4.38983624e-04,
        4.14804312e-02, -5.94012921e-02,  4.49523926e-02, -3.12969501e-02,
       -6.20962290e-02,  2.44891827e-02, -1.09238845e-01,  7.57915790e-02,
       -6.23497596e-03,  6.58176129e-02,  1.60287710e-02, -1.07046274e-02,
       -1.47141677e-02, -8.01579402e-02,  5.68002554e-02, -1.18774414e-01,
        7.04251803e-03,  1.09637921e-01, -6.26314603e-02, -5.73120117e-02,
        2.55995530e-02, -3.46773588e-02, -2.95973558e-02,  1.12135667e-01,
       -1.24771118e-01,  6.16924579e-02,  9.30880033e-02,  5.36452073e-02,
        3.64285983e-02, -9.43392240e-02, -1.51986929e-01, -8.02189754e-02,
        1.04764498e-01,  1.33020254e-01, -1.31695087e-02,  8.83225661e-02,
       -9.19893705e-02, -3.18861741e-02, -4.56859882e-02, -2.59164663e-02,
        5.09502704e-02, -8.04725060e-03, -1.69208233e-02,  1.70917218e-01,
        2.82639724e-02,  1.57552866e-02,  1.88504733e-02,  5.26281504e-02,
       -5.15717726e-02, -1.13628681e-01, -2.30806791e-02, -5.21756686e-02,
        7.92518029e-02,  5.03751315e-02,  8.04631160e-02,  2.58225661e-03,
       -4.53256460e-02,  6.01431040e-03, -3.62267127e-02,  5.01990685e-02,
       -9.67665452e-02,  3.82024325e-02, -3.22171725e-02, -1.56250000e-02,
        7.71484375e-02, -3.07218111e-02, -1.22483474e-01, -9.05879094e-02,
       -3.23814979e-02,  1.17351825e-02,  6.29841731e-02,  5.60044509e-02,
        3.09236967e-02, -3.36092435e-02,  2.68625113e-02,  7.35048147e-02,
       -4.62458684e-02,  2.31370192e-02, -6.49367112e-02,  6.74907978e-02,
        3.49872296e-02, -6.85644883e-02, -3.53346605e-02, -1.73715445e-04,
        4.26025391e-02, -2.24750225e-02,  1.23502291e-02, -5.67908654e-02,
       -6.55235877e-02, -1.71227088e-02, -4.63547340e-02, -3.40441190e-02,
        1.25098595e-02,  7.15989333e-03,  9.31161734e-02,  7.35567533e-02,
        1.33526142e-01, -1.33573092e-01,  8.24303260e-02,  8.55431190e-03,
        4.03864934e-02,  1.00848858e-02, -2.84517728e-02, -9.41678561e-02,
        2.37567608e-02, -8.44632662e-03,  3.24049730e-02, -1.96932279e-02,
       -1.09628531e-01, -5.23024339e-03, -6.00022536e-03,  2.62920673e-03,
       -8.03833008e-02,  4.89220252e-02, -9.26595835e-02,  3.14237154e-02,
       -4.43443885e-02,  5.74481671e-02,  1.29676232e-02, -1.17375300e-03,
        3.23333740e-02, -1.45014836e-01,  6.45094651e-03,  2.05735427e-02,
       -1.76062951e-03,  5.49081656e-03, -1.90129207e-01, -7.14956430e-02,
       -5.39926382e-02, -6.09036959e-02, -4.35791016e-02, -5.30771109e-02,
        9.03038612e-02, -4.26518367e-02,  2.79001089e-02,  3.19824219e-02,
       -2.84611629e-02, -1.90570538e-02, -9.09775954e-03,  5.87064303e-02,
       -3.32782452e-02,  1.26953125e-02, -3.67330404e-02,  1.74842248e-02,
       -2.76536208e-03,  9.14705717e-02,  6.97490986e-02,  2.19257061e-02,
       -9.62101863e-02,  1.88856858e-02,  1.33361816e-02, -5.05418044e-02,
       -2.40672185e-02, -7.64817458e-02, -7.65568660e-02, -9.56373948e-02,
       -3.26655461e-02,  6.78147536e-02, -9.52148438e-03, -7.49323918e-03,
        2.35126202e-02,  1.79349459e-02, -3.15692608e-02, -3.87056791e-02,
       -2.27391170e-02,  2.40196815e-02, -1.13912729e-03,  4.28372897e-02,
       -4.05555138e-02, -2.44985727e-02, -9.10175030e-02, -1.79842435e-02,
        1.35047326e-01, -5.96782978e-02, -1.57883864e-01,  4.77201022e-02,
       -1.91744291e-02,  3.54942909e-02, -6.94298377e-02, -2.39633413e-02,
        5.86876502e-02, -2.69024189e-03,  8.16826454e-02,  9.09893329e-02,
       -3.54121282e-02,  2.68930288e-02,  3.04729755e-02, -4.98985877e-02,
        9.03555063e-03,  7.77306190e-02,  9.14685176e-02, -4.23306685e-02,
        6.25751202e-02, -2.02073317e-02,  8.46604567e-02,  4.39805251e-03,
        5.88003305e-02,  2.05218975e-02,  7.59183444e-03, -6.61973220e-02,
        2.18881460e-02,  7.80193622e-03,  1.47000826e-02,  6.03355995e-02,
        4.22175481e-02,  1.35028546e-02,  7.33971229e-02,  3.12500000e-02,
        8.58927507e-02,  6.22136043e-02,  5.06403996e-02, -1.01662856e-01,
        4.24382136e-02,  6.73452524e-02, -2.33060397e-02, -4.59641677e-02,
        4.77670523e-02, -3.11654898e-02, -1.66485126e-02,  3.61738939e-02,
       -1.52587891e-03,  1.18326040e-01, -3.67901142e-02,  1.15039532e-02,
        9.46514423e-03, -6.43967849e-02,  3.15129207e-02,  4.30344802e-02,
        8.95104041e-02,  7.76461088e-02,  8.36111215e-02, -1.35286771e-02,
       -7.02655499e-02, -1.11309345e-01,  1.11553486e-02, -3.89216496e-03,
       -2.73061899e-02,  2.83766526e-02, -2.83437876e-02,  3.96728516e-03,
        6.20563214e-03,  3.27524038e-02, -5.62239427e-02,  2.61981671e-03,
        1.04135367e-02,  3.98137019e-03, -9.35058594e-02,  3.24472281e-02,
       -1.69800978e-02,  2.03904372e-02, -6.92702073e-02,  4.45087139e-03,
       -9.13813664e-02, -7.45286208e-02, -8.67550190e-03, -7.15519832e-03])

```python
len(X_train_word2vec[2])
```
300

```python
fig = plt.figure(figsize=(16,16))
plot_LSA(embeddings, list_labels)
plt.show()
```
![image](https://github.com/user-attachments/assets/4a400724-bd4e-4a75-8bb9-13b528928409)

**ä»å¯è§†åŒ–å›¾ä¸Šæ¥çœ‹ï¼Œä¼¼ä¹å˜å¾—çº¿æ€§å¯åˆ†äº†ã€‚**

```python
clf_w2v = LogisticRegression(C=30.0, class_weight='balanced', solver="newton-cg",
                         multi_class='multinomial', n_jobs=-1, random_state=40)
clf_w2v.fit(X_train_word2vec, y_train_word2vec)

y_predicted_word2vec = clf_w2v.predict(X_test_word2vec)
```

```python
accuracy_word2vec, precision_word2vec, recall_word2vec, f1_word2vec = get_metrics(y_test_word2vec, y_predicted_word2vec)
print("accuracy: %.3f, precision: %.3f, recall: %.3f, f1: %.3f" % (accuracy_word2vec, 
                                                                   precision_word2vec, 
                                                                   recall_word2vec, 
                                                                   f1_word2vec))
```
accuracy: 0.778, precision: 0.777, recall: 0.778, f1: 0.777  

```python
cm = confusion_matrix(y_test_word2vec, y_predicted_word2vec)
fig = plt.figure(figsize=(10,10))
plot = plot_confusion_matrix(cm, classes=['Irrelevant', 'Disaster', 'Unsure'], title="Confusion Matrix")
plt.show()
```
![image](https://github.com/user-attachments/assets/3b84a8af-7f6a-42ce-85cb-84ca9f02c146)

**è¯åµŒå…¥åœ¨åˆ†ç±»çš„æ—¶å€™ï¼Œæ˜¯ç›´æ¥å°†ä¸€å¥è¯ä¸­çš„å•è¯çš„è¯å‘é‡è¿›è¡Œå¹³å‡ç„¶åå†è¿›è¡Œåˆ†ç±»ï¼Œå¹¶æ²¡æœ‰è€ƒè™‘ä¸€å¥è¯ä¸­å•è¯ä¹‹é—´çš„åºåˆ—ï¼ˆå‰åé¡ºåºï¼‰**

### 3.5 åŸºäºæ·±åº¦å­¦ä¹ çš„è¯åµŒå…¥ï¼ˆCNN or RNNï¼‰
```python
from keras_preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from keras.utils import to_categorical
```

```python
EMBEDDING_DIM = 300
MAX_SEQUENCE_LENGTH = 35
VOCAB_SIZE = len(VOCAB)
```

```python
VALIDATION_SPLIT = .2
# åŸºäºè¯é¢‘ï¼Œé¢‘ç‡æœ€é«˜çš„å•è¯ä¼šæ’åœ¨å‰é¢ï¼Œå°†æ–‡æœ¬æ•°æ®è½¬æ¢ä¸ºæ•´æ•°åºåˆ—
tokenizer = Tokenizer(num_words=VOCAB_SIZE)
tokenizer.fit_on_texts(clean_data["text"].tolist())
sequences = tokenizer.texts_to_sequences(clean_data["text"].tolist())
```

```python
word_index = tokenizer.word_index
print("Found %s unique tokens." % len(word_index))
```
Found 19097 unique tokens.  

```python
cnn_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)
labels = to_categorical(np.asarray(clean_data["class_label"]))
```

```python
indices = np.arange(cnn_data.shape[0])
np.random.shuffle(indices)
cnn_data = cnn_data[indices]
labels = labels[indices]
num_validation_samples = int(VALIDATION_SPLIT * cnn_data.shape[0])

embedding_weights = np.zeros((len(word_index)+1, EMBEDDING_DIM))
for word,index in word_index.items():
    embedding_weights[index,:] = word2vec[word] if word in word2vec else np.random.rand(EMBEDDING_DIM)
print(embedding_weights.shape)
```
(19098, 300)  

```python
from keras.layers import Dense, Input, Flatten, Dropout, Concatenate  # ä½¿ç”¨ Concatenate æ›¿ä»£ Merge
from keras.layers import Conv1D, MaxPooling1D, Embedding  # æ³¨æ„ MaxPooling1D çš„å¤§å°å†™
from keras.layers import LSTM, Bidirectional
from keras.models import Model  # Model åº”è¯¥ä» keras.models å¯¼å…¥
```

```python
def ConvNet(embeddings, max_sequence_length,
            num_words, embedding_dim, lables_index, trainable=False, extra_conv=True):
    embedding_layer = Embedding(num_words, embedding_dim, weights=[embeddings],
                                input_length=max_sequence_length,
                                trainable=trainable)
    sequences_input = Input(shape=(max_sequence_length,), dtype='int32')
    embedded_sequences = embedding_layer(sequences_input)

    convs = []
    filter_size = [3,4,5]
    
    for filter_size in filter_size:
        l_conv = Conv1D(filters=128, kernel_size=filter_size, activation='relu')(embedded_sequences)
        l_pool = MaxPooling1D(pool_size=3)(l_conv)
        convs.append(l_pool)

    l_merge = Concatenate(axis=1)(convs)

    conv = Conv1D(filters=128, kernel_size=3, activation='relu')(embedded_sequences)
    pool = MaxPooling1D(pool_size=3)(conv)

    if extra_conv == True:
        x = Dropout(0.5)(l_merge)
    else:
        x = Dropout(0.5)(pool)
    
    x = Flatten()(x)
    x = Dense(128, activation='relu')(x)

    preds = Dense(lables_index, activation='softmax')(x)

    model = Model(sequences_input, preds)
    model.compile(loss='categorical_crossentropy',
                  optimizer='adam',
                  metrics=['acc'])
    
    return model
```

```python
x_train = cnn_data[:-num_validation_samples]
y_train = labels[:-num_validation_samples]
x_val = cnn_data[-num_validation_samples:]
y_val = labels[-num_validation_samples:]

model = ConvNet(embedding_weights, MAX_SEQUENCE_LENGTH, len(word_index)+1, EMBEDDING_DIM,
                len(list(clean_data["class_label"].unique())),
                False)
```

```python
model.fit(x_train, y_train, validation_data=(x_val, y_val), epochs=3, batch_size=128)
```
Epoch 1/3
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m9s[0m 104ms/step - acc: 0.6421 - loss: 0.6879 - val_acc: 0.7522 - val_loss: 0.5103
Epoch 2/3
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m3s[0m 43ms/step - acc: 0.7873 - loss: 0.4667 - val_acc: 0.7936 - val_loss: 0.4641
Epoch 3/3
[1m68/68[0m [32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”[0m[37m[0m [1m2s[0m 28ms/step - acc: 0.8312 - loss: 0.3899 - val_acc: 0.7959 - val_loss: 0.4610
